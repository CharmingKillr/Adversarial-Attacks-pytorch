{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-14T13:20:44.710579Z",
     "start_time": "2024-09-14T13:20:44.695045Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([3, 5]), tensor([2, 2]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word embedding 以序列建模为例\n",
    "# 考虑source sentence 和 target sentence\n",
    "# 构建序列，序列的字符以其在词表中的索引的形式表示\n",
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "\n",
    "# 词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_embedding_dim = 8 #原文为512\n",
    "\n",
    "\n",
    "# 序列的最大长度   超参数 \n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "\n",
    "src_len = torch.randint(2, 6, (batch_size,))\n",
    "tgt_len = torch.randint(2, 6, (batch_size,))\n",
    "src_len,tgt_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T12:56:37.267407Z",
     "start_time": "2024-09-14T12:56:37.252407Z"
    }
   },
   "id": "22e1cbb210da4ab8",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 4, 5, 0, 0],\n",
      "        [2, 4, 3, 5, 7]]) torch.Size([2, 5])\n",
      "tensor([[3, 6],\n",
      "        [6, 3]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 单词索引构成源句子和目标句子，构建batch, 做了padding， 默认值为0\n",
    "src_seq =[torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max(src_len)- L)), 0) \\\n",
    "          for L in src_len]\n",
    "tgt_seq =[torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max(tgt_len)- L)), 0) \\\n",
    "          for L in tgt_len]\n",
    "\n",
    "src_seq = torch.cat(src_seq, dim=0)\n",
    "tgt_seq = torch.cat(tgt_seq, dim=0)\n",
    "\n",
    "print(src_seq, src_seq.shape)\n",
    "print(tgt_seq, tgt_seq.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T12:57:17.192197Z",
     "start_time": "2024-09-14T12:57:17.173104Z"
    }
   },
   "id": "8e33be9552a62ff5",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2053,  0.3051,  0.5357, -0.4312,  0.1573,  1.2540,  1.3275, -0.4954],\n",
      "        [-1.9804,  1.7986,  0.1018,  0.3400, -0.6447, -0.2870,  3.3212, -0.4021],\n",
      "        [-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085, -0.2155],\n",
      "        [ 2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484, -0.3472],\n",
      "        [ 0.4730, -0.4286,  0.5514, -1.5474,  0.7575, -0.4068, -0.1277,  0.2804],\n",
      "        [ 1.7460,  1.8550, -0.7064,  2.5571,  0.7705, -1.0739, -0.2015, -0.5603],\n",
      "        [-0.6240, -0.9773,  0.8748,  0.9873,  0.2505, -0.7930,  0.5231,  1.2236],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038, -0.5685,  0.8376,  1.7837, -0.1954],\n",
      "        [-1.1435, -0.6512, -0.1032,  0.6937, -0.5413,  0.8952, -0.8825,  0.5318]],\n",
      "       requires_grad=True)\n",
      "tensor([[[ 2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484,\n",
      "          -0.3472],\n",
      "         [-0.6240, -0.9773,  0.8748,  0.9873,  0.2505, -0.7930,  0.5231,\n",
      "           1.2236]],\n",
      "\n",
      "        [[-0.6240, -0.9773,  0.8748,  0.9873,  0.2505, -0.7930,  0.5231,\n",
      "           1.2236],\n",
      "         [ 2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484,\n",
      "          -0.3472]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 构造word embedding \n",
    "src_embedding_table = nn.Embedding(max_num_src_words + 1, model_embedding_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_embedding_dim)\n",
    "print(tgt_embedding_table.weight)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "print(tgt_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T12:57:43.736087Z",
     "start_time": "2024-09-14T12:57:43.726480Z"
    }
   },
   "id": "56fb88e0e419f98b",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 构造position embedding\n",
    "pos_mat = torch.arange(max_position_len).reshape(-1, 1)\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape(1, -1) / model_embedding_dim)\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_embedding_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "# 位置索引\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len])\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in src_len])\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len, model_embedding_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T12:57:59.972141Z",
     "start_time": "2024-09-14T12:57:59.958234Z"
    }
   },
   "id": "d1e3d3e9dbda0bf5",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[   1.,   10.,  100., 1000.]]),\n tensor([[0],\n         [1],\n         [2],\n         [3],\n         [4]]),\n tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n         [1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03],\n         [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03],\n         [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n         [4.0000e+00, 4.0000e-01, 4.0000e-02, 4.0000e-03]]),\n Parameter containing:\n tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n           9.9995e-01,  1.0000e-03,  1.0000e+00],\n         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n           9.9980e-01,  2.0000e-03,  1.0000e+00],\n         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n           9.9955e-01,  3.0000e-03,  1.0000e+00],\n         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n           9.9920e-01,  4.0000e-03,  9.9999e-01]]),\n tensor([[0, 1, 2, 3, 4],\n         [0, 1, 2, 3, 4]]),\n tensor([[0, 1],\n         [0, 1]]),\n tensor([[[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n           1.0000e+00, 0.0000e+00, 1.0000e+00],\n          [8.4147e-01, 5.4030e-01, 9.9833e-02, 9.9500e-01, 9.9998e-03,\n           9.9995e-01, 1.0000e-03, 1.0000e+00]],\n \n         [[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n           1.0000e+00, 0.0000e+00, 1.0000e+00],\n          [8.4147e-01, 5.4030e-01, 9.9833e-02, 9.9500e-01, 9.9998e-03,\n           9.9995e-01, 1.0000e-03, 1.0000e+00]]]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_mat , pos_mat, pos_mat / i_mat, pe_embedding.weight, src_pos, tgt_pos, tgt_pe_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T11:59:58.552365Z",
     "start_time": "2024-09-14T11:59:58.530362Z"
    }
   },
   "id": "fe1db550fccbb06c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "tensor([[[False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False, False],\n",
      "         [False, False, False, False, False],\n",
      "         [False, False, False, False, False],\n",
      "         [False, False, False, False, False],\n",
      "         [False, False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "# 构造encoder的self-attention mask\n",
    "# mask的shape: [batch_size, max_src_len, max_src_len],值为1或为-inf\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len) - L)), 0) \\\n",
    "                               for L in src_len]), 2)\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "print(invalid_encoder_pos_matrix)\n",
    "print(mask_encoder_self_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T13:16:11.910131Z",
     "start_time": "2024-09-14T13:16:11.893728Z"
    }
   },
   "id": "b465e00af17c60e",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0757, -0.5536, -1.6160,  0.0934, -1.3898],\n",
      "         [-0.3105,  1.0693,  1.4394,  1.3694,  0.4539],\n",
      "         [-0.0498,  0.3745,  1.4389,  1.4151, -0.1589],\n",
      "         [-0.7360, -2.0311, -1.1064,  0.1879,  0.1146],\n",
      "         [ 1.1904,  0.5201, -0.3884, -0.0316, -1.1085]],\n",
      "\n",
      "        [[-1.8874, -0.0863,  1.4791,  0.9428, -0.1244],\n",
      "         [-1.5083, -0.7200,  0.7923, -0.3385,  1.4161],\n",
      "         [ 0.4738,  0.0827, -1.3034,  0.5190,  1.3395],\n",
      "         [-0.2388, -0.0680,  1.1349,  0.8658,  0.6334],\n",
      "         [-0.5392,  0.0182,  1.4142, -1.7438,  0.1129]]])\n",
      "tensor([[[ 1.0757e+00, -5.5361e-01, -1.6160e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-3.1046e-01,  1.0693e+00,  1.4394e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-4.9785e-02,  3.7450e-01,  1.4389e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[-1.8874e+00, -8.6326e-02,  1.4791e+00,  9.4282e-01, -1.2443e-01],\n",
      "         [-1.5083e+00, -7.1996e-01,  7.9233e-01, -3.3846e-01,  1.4161e+00],\n",
      "         [ 4.7376e-01,  8.2703e-02, -1.3034e+00,  5.1899e-01,  1.3395e+00],\n",
      "         [-2.3876e-01, -6.7961e-02,  1.1349e+00,  8.6577e-01,  6.3335e-01],\n",
      "         [-5.3920e-01,  1.8176e-02,  1.4142e+00, -1.7438e+00,  1.1289e-01]]])\n",
      "tensor([[[0.7912, 0.1551, 0.0536, 0.0000, 0.0000],\n",
      "         [0.0932, 0.3704, 0.5363, 0.0000, 0.0000],\n",
      "         [0.1437, 0.2196, 0.6367, 0.0000, 0.0000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
      "\n",
      "        [[0.0170, 0.1030, 0.4927, 0.2882, 0.0991],\n",
      "         [0.0286, 0.0628, 0.2850, 0.0920, 0.5317],\n",
      "         [0.1898, 0.1284, 0.0321, 0.1986, 0.4511],\n",
      "         [0.0866, 0.1027, 0.3421, 0.2614, 0.2072],\n",
      "         [0.0832, 0.1453, 0.5868, 0.0249, 0.1597]]])\n"
     ]
    }
   ],
   "source": [
    "# score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "# \n",
    "# masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "# prob = torch.softmax(masked_score, dim=-1)\n",
    "# \n",
    "# print(score)\n",
    "# print(masked_score)\n",
    "# print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T13:21:29.756349Z",
     "start_time": "2024-09-14T13:21:29.745827Z"
    }
   },
   "id": "a5d4c75fcb3ad10d",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # scaled的重要性\n",
    "# alpha1 = 0.1\n",
    "# alpha2 = 10\n",
    "# score = torch.randn(5)\n",
    "# prob1 = F.softmax(score*alpha1, dim=-1)\n",
    "# prob2 = F.softmax(score*alpha2, dim=-1)\n",
    "# print(score)\n",
    "# print(prob1, prob2)\n",
    "# \n",
    "# def softmax_func(score):\n",
    "#     return F.softmax(score, dim=-1)\n",
    "# \n",
    "# jaco_mat1 = torch.autograd.functional.jacobian(softmax_func, score*alpha1)\n",
    "# jaco_mat2 = torch.autograd.functional.jacobian(softmax_func, score*alpha2)\n",
    "# print(jaco_mat1)\n",
    "# print(jaco_mat2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-14T12:01:27.661279Z",
     "start_time": "2024-09-14T12:01:27.646828Z"
    }
   },
   "id": "4c22caed1a96a334",
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
